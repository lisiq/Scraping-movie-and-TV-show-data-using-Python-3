{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = {\n",
    "  'http': 'http://92.119.177.90',\n",
    "}\n",
    "url = 'https://www.tntdrama.com'\n",
    "urlshows = 'https://www.tntdrama.com/shows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the shows\n",
    "r = requests.get(urlshows, proxies = proxies)\n",
    "c = r.content\n",
    "soup = BeautifulSoup(c, 'html.parser')\n",
    "result = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if a['href'][:7] == '/shows/':\n",
    "        result.append(a['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get episodes\n",
    "episodes = []\n",
    "for i in result:\n",
    "    r = requests.get(url + i + '/watch-now', proxies = proxies)\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a['href'].startswith(i+'/s'):\n",
    "            episodes.append(a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving episode urls\n",
    "episodes = list(set(episodes))\n",
    "for i in episodes:\n",
    "    with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/episodes.txt', 'a') as f:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/episodes.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info from episodes\n",
    "\n",
    "for i in lines:\n",
    "    r = requests.get(url + i, proxies = proxies)\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_ldjson/' + i.encode(\"utf-8\").hex() + '.json', 'w') as j:\n",
    "        j.write(json.dumps(\n",
    "            str(soup.find_all(lambda tag: tag.name == 'script' and \n",
    "                              tag.get('type') == 'application/ld+json'))[36:-10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for i in lines:\n",
    "    try:\n",
    "        with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_ldjson/' + i.encode(\"utf-8\").hex() + '.json', 'r') as j:\n",
    "            dict_list.append(dict(eval(json.load(j).replace('null', '\"NaN\"')\\\n",
    "                                           .replace('true',\"True\")\\\n",
    "                                           .replace('false',\"False\"))))\n",
    "    except SyntaxError:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(n):\n",
    "    m = n['potentialAction'][0]\n",
    "    url = \"\\\\/\" + \"/\".join(n['@id'].split(\"/\", 4)[3:]) \n",
    "    genre = m['@type']\n",
    "    language = m['target']['inLanguage']\n",
    "    bot_country = m['actionAccessibilityRequirement']['eligibleRegion']['name']\n",
    "    availabilityStarts = m['actionAccessibilityRequirement']['availabilityStarts']\n",
    "    availabilityEnds = m['actionAccessibilityRequirement']['availabilityEnds']\n",
    "    bot_system = m['actionAccessibilityRequirement']['requiresSubscription']['name']\n",
    "    offer_type = m['actionAccessibilityRequirement']['requiresSubscription']['authenticator']['name']\n",
    "    return [url, genre, language, bot_country, availabilityStarts, availabilityEnds, bot_system, offer_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for i in dict_list:\n",
    "    try:\n",
    "        flat_list.append(get_info(i))\n",
    "    except KeyError:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tntdramashows_ldjson = pd.DataFrame(flat_list, \n",
    "                               columns=['url', 'genre', 'language', 'bot_country', 'availabilityStarts', \n",
    "                                        'availabilityEnds', 'bot_system', 'offer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tntdramashows_ldjson.to_csv('/home/lisi/Desktop/MediaBiz/TNTDRAMA/tntdramashows_ldjson.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
