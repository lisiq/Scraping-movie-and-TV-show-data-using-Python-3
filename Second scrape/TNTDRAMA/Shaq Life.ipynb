{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "# we first get a US proxy and the web-page that we want to scrape\n",
    "proxies = {\n",
    "  'http': 'http://92.119.177.92',\n",
    "}\n",
    "url = 'https://www.tntdrama.com'\n",
    "urlshows = 'https://www.tntdrama.com/shaq-life/shaq-life-watch-episodes'\n",
    "\n",
    "\n",
    "episodes = []\n",
    "r = requests.get(urlshows, proxies = proxies)\n",
    "c = r.content\n",
    "soup = BeautifulSoup(c, 'html.parser')\n",
    "for a in soup.find_all(lambda tag: tag.name == 'a' and tag.get('title') == 'Shaq Life'):\n",
    "        episodes.append(a['href'])\n",
    "\n",
    "\n",
    "# we remove the duplicated links\n",
    "# save all the links for furute needs\n",
    "episodes = list(set(episodes))\n",
    "for i in episodes:\n",
    "    with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/episodes.txt', 'a') as f:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "\n",
    "with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/episodes.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    \n",
    "\n",
    "# # open all the links and scrape the json file that has some data about the series\n",
    "# # and save them so we do not have to redowload them if we make some mistakes in the future\n",
    "for i in lines:\n",
    "    r = requests.get(url + i, proxies = proxies)\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_ldjson/' + i.encode(\"utf-8\").hex() + '.json', 'w') as j:\n",
    "        j.write(json.dumps(\n",
    "            str(soup.find_all(lambda tag: tag.name == 'script' and \n",
    "                              tag.get('type') == 'application/ld+json'))[36:-10]))\n",
    "\n",
    "\n",
    "# we open the json files and make the syntax python friendly\n",
    "# after that we transofrm them into python dictionaries\n",
    "# and save them in a list. There can be some json files \n",
    "# that do not have the info that we need so we ignore them\n",
    "dict_list = []\n",
    "for i in lines:\n",
    "    try:\n",
    "        with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_ldjson/' + i.encode(\"utf-8\").hex() + '.json', 'r') as j:\n",
    "            dict_list.append(dict(eval(json.load(j).replace('null', '\"NaN\"')\\\n",
    "                                           .replace('true',\"True\")\\\n",
    "                                           .replace('false',\"False\"))))\n",
    "    except SyntaxError:\n",
    "        pass\n",
    "\n",
    "\n",
    "# # we define this helper function to get only the data \n",
    "# # that we need form the dictionaries\n",
    "# def get_info(n):\n",
    "#     m = n['potentialAction'][0]\n",
    "#     url = \"\\\\/\" + \"/\".join(n['@id'].split(\"/\", 4)[3:]) \n",
    "#     genre = m['@type']\n",
    "#     language = m['target']['inLanguage']\n",
    "#     bot_country = m['actionAccessibilityRequirement']['eligibleRegion']['name']\n",
    "#     availabilityStarts = m['actionAccessibilityRequirement']['availabilityStarts']\n",
    "#     availabilityEnds = m['actionAccessibilityRequirement']['availabilityEnds']\n",
    "#     bot_system = m['actionAccessibilityRequirement']['requiresSubscription']['name']\n",
    "#     offer_type = m['actionAccessibilityRequirement']['requiresSubscription']['authenticator']['name']\n",
    "#     return [url, genre, language, bot_country, availabilityStarts, availabilityEnds, bot_system, offer_type]\n",
    "\n",
    "# # we apply the helper function to all the dictionaries \n",
    "# # and ignore the files that do not have the data that we need\n",
    "# flat_list = []\n",
    "# for i in dict_list:\n",
    "#     try:\n",
    "#         flat_list.append(get_info(i))\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "        \n",
    "# # here we create a data fram from the list overhead and name the columns\n",
    "# tntdramashows_ldjson = pd.DataFrame(flat_list, \n",
    "#                                columns=['url', 'genre', 'language', 'bot_country', 'availabilityStarts', \n",
    "#                                         'availabilityEnds', 'bot_system', 'offer_type'])\n",
    "\n",
    "# # finally we save the data as a csv file\n",
    "# tntdramashows_ldjson.to_csv('/home/lisi/Desktop/MediaBiz/TNTDRAMA/tntdramashows_ldjson.csv', index=False)\n",
    "\n",
    "# there is another json file that contain some more information\n",
    "# we will scrape also that\n",
    "# to scrape this we do not need all the link, we just need a link per season\n",
    "# since it containas information for every other episode\n",
    "# new_lines = []\n",
    "# for i in lines:\n",
    "#     match = \"/\".join(i.split(\"/\", 4)[:4]) #split until fourth occurence of /, and then get first 4 elements, after that join\n",
    "#     if match not in new_lines:\n",
    "#         new_lines.append(match)\n",
    "#         new_lines.append(i)\n",
    "\n",
    "# new_lines = new_lines[1::2]\n",
    "\n",
    "# we scrape all the json files and save them \n",
    "for i in lines:\n",
    "    r = requests.get(url + i, proxies = proxies)\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_drupla-settings-json/' + i.encode(\"utf-8\").hex() + '.json', 'w') as j:\n",
    "        j.write(json.dumps(\n",
    "            str(soup.find_all(lambda tag: tag.name == 'script' and \n",
    "                              tag.get('type') == 'application/json' and\n",
    "                            tag.get('data-drupal-selector') == 'drupal-settings-json')[0])[76:-9]))\n",
    "\n",
    "# we open the files transform them into dictionaries, make the syntax python friendly\n",
    "# and ignore the files that do not have the data that we need\n",
    "flat_list = []\n",
    "dict_list = []\n",
    "for i in lines:\n",
    "    try:\n",
    "        with open('/home/lisi/Desktop/MediaBiz/TNTDRAMA/TNTDRAMAshows_drupla-settings-json/' + i.encode(\"utf-8\").hex() + '.json', 'r') as j:\n",
    "            dict_list.append(dict(eval(json.load(j).replace('null', '\"NaN\"')\\\n",
    "                                       .replace('true',\"True\")\\\n",
    "                                       .replace('false',\"False\")))['turner_playlist'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "# # transform from a nested list into a list         \n",
    "# flat_list = [item for sublist in dict_list for item in sublist]\n",
    "\n",
    "# # create a dataframe\n",
    "# tntdramashows_drupla_settings_json = pd.DataFrame(flat_list)\n",
    "\n",
    "# # finally save it\n",
    "# tntdramashows_drupla_settings_json.to_csv('/home/lisi/Desktop/MediaBiz/TNTDRAMA/tntdramashows_drupla_settings_json.csv', index=False)\n",
    "\n",
    "# # save the merger\n",
    "# df_tntdramashows = pd.merge(tntdramashows_drupla_settings_json, tntdramashows_ldjson, how='left', on='url')\n",
    "\n",
    "# df_tntdramashows.to_csv('/home/lisi/Desktop/MediaBiz/TNTDRAMA/tntdramashows.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
